{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Flight Radar 24 \n",
    "\n",
    "Overview: \n",
    "- You are developing a system to process and analyze airport and flight data in real-time. Your objective is to design and implement a scalable solution for analyzing large datasets generated by various sources, e.g. FlightRadar.\n",
    "\n",
    "Task:\n",
    "- Load a dataset simulating airport and flight data - adsb.json, oag.jsonfiles.\n",
    "- Use Apache Spark to ingest and process the data (e.g., data cleaning, aggregation, transformation).\n",
    "- Conduct simple analysis. Compute some basic airport KPIs, including but not limited to:\n",
    "        - average speed for each airport \n",
    "        - the total number of delayed flights (categorized into arrival delays and departure delays)\n",
    "- Filter and transform a DataFrame by applying a window function (Spark partitioning):\n",
    "        - Filter the DataFrame to retain only the most recent entry (the one with the smallest LastUpdate) for each FlightId.\n",
    "        - Return a DataFrame containing only the FlightId and the corresponding latest LastUpdate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import explode,count, col, isnan, date_format, from_unixtime, avg, row_number,when\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder \n",
    "        .appName(\"Flight Radar 24\") \n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Load a dataset simulating airport and flight data: \n",
    " - adsb.json, \n",
    " - oag.jsonfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    adsb_df = spark.read.json(\"./data/adsb.json\", multiLine=True)\n",
    "    print(\"ADS-B data loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ADS-B data: {e}\")\n",
    "\n",
    "try:\n",
    "    oag_df = spark.read.json(\"./data/oag.json\", multiLine=True)\n",
    "    print(\"OAG data loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading OAG data: {e}\")\n",
    "\n",
    "try:\n",
    "    flights_df = oag_df.select(explode(\"data\").alias(\"flight\"))\n",
    "    print(\"Flight data processed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing flight data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Data Processing \n",
    "- Use Apache Spark to ingest and process the data (e.g., data cleaning, aggregation, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows and columns\n",
    "print(f'The datase \"adsb\" has {adsb_df.count()} rows and {len(adsb_df.columns)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in adsb_df.columns:\n",
    "    unique_count = adsb_df.select(column).distinct().count()\n",
    "    print(f\"The column '{column}' has {unique_count} unique values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for both null and NaN values in each column\n",
    "null_nan_check = adsb_df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in adsb_df.columns])\n",
    "null_nan_check.show()\n",
    "# there are 6 missing values in RadarId but i will not delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with invalid Speed (negative values)\n",
    "adsb_df = adsb_df.filter(adsb_df[\"Speed\"] >= 0)\n",
    "adsb_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The measure of flight speed are in knots (1knot = 1,852 km/h)\n",
    "# convert speed \n",
    "try:\n",
    "    adsb_df = adsb_df.withColumn(\"SpeedKmH\", round(col(\"Speed\") * 1.852, 2))\n",
    "    \n",
    "    print(\"Speed conversion and rounding successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during speed conversion and rounding: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the LastUpdate column\n",
    "try:\n",
    "    adsb_df = adsb_df.withColumn(\"LastUpdate\", date_format(from_unixtime(\"LastUpdate\"), \"yyyy-MM-dd HH:mm\"))\n",
    "    print(\"LastUpdate column formatted successfully.\")\n",
    "\n",
    "except AnalysisException as ae:\n",
    "    print(f\"AnalysisException occurred: {ae}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the Flight column to FlightId\n",
    "try:\n",
    "    adsb_df = adsb_df.withColumnRenamed(\"Flight\", \"FlightId\")\n",
    "    print(\"Column renamed successfully from 'Flight' to 'FlightId'.\")\n",
    "\n",
    "except AnalysisException as ae:\n",
    "    print(f\"AnalysisException occurred: {ae}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Basic airport KPIs: \n",
    " - average speed for each airport \n",
    " - the total number of delayed flights (categorized into arrival delays and departure delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_airport= adsb_df.select(\"Origin\").distinct()\n",
    "unique_airport_list = [row[\"Origin\"] for row in unique_airport.collect()]\n",
    "\n",
    "print(f'The dataset \"adbs_df\" has {unique_airport.count()} unique airports.')\n",
    "print(\"The unique airports are:\")\n",
    "print(\", \".join(unique_airport_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_adsb_df = adsb_df.filter(adsb_df[\"Onground\"] == 0)\n",
    "average_speed = filtered_adsb_df.groupBy(\"Origin\").agg(avg(\"Speed\").alias(\"AverageSpeed\"))\n",
    "average_speed_results = average_speed.collect()\n",
    "\n",
    "for row in average_speed_results:\n",
    "    origin_airport = row[\"Origin\"]\n",
    "    avg_speed = row[\"AverageSpeed\"]\n",
    "    print(f\"The average speed for flights from {origin_airport} (in the air) is {avg_speed:.2f} knot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_adsb_df = adsb_df.filter(adsb_df[\"Onground\"] == 0)\n",
    "average_speed = filtered_adsb_df.groupBy(\"Origin\").agg(avg(\"SpeedKmH\").alias(\"AverageSpeedKmH\"))\n",
    "average_speed_results = average_speed.collect()\n",
    "\n",
    "for row in average_speed_results:\n",
    "    origin_airport = row[\"Origin\"]\n",
    "    avg_speed = row[\"AverageSpeedKmH\"]\n",
    "    print(f\"The average speed for flights from {origin_airport} (in the air) is {avg_speed:.2f} km/h.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = flights_df.select(\n",
    "    col(\"flight.statusDetails.arrival.actualTime.inGateTimeliness\").alias(\"arrival_inGateTimeliness\"),\n",
    "    col(\"flight.statusDetails.departure.actualTime.outGateTimeliness\").alias(\"departure_outGateTimeliness\")\n",
    ")\n",
    "\n",
    "\n",
    "flights_df = flights_df.withColumn(\"arrival_inGateTimeliness\", col(\"arrival_inGateTimeliness\").getItem(0))\n",
    "flights_df = flights_df.withColumn(\"departure_outGateTimeliness\", col(\"departure_outGateTimeliness\").getItem(0))\n",
    "\n",
    "\n",
    "print(f\"Total number of delayed arrival flights: {flights_df.filter(col(\"arrival_inGateTimeliness\") == \"Delayed\").count()}\")\n",
    "print(f\"Total number of delayed departure flights: {flights_df.filter(col(\"departure_outGateTimeliness\") == \"Delayed\").count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Filter and transform a DataFrame by applying a window function (Spark partitioning):\n",
    " - Filter the DataFrame to retain only the most recent entry (the one with the smallest LastUpdate) for each FlightId.\n",
    " - Return a DataFrame containing only the FlightId and the corresponding latest LastUpdate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_adsb_df = Window.partitionBy(\"FlightId\").orderBy(\"LastUpdate\")\n",
    "\n",
    "adsb_latest_df = (adsb_df\n",
    "                  .withColumn(\"row_num\", row_number().over(order_adsb_df)) \n",
    "                  .filter(col(\"row_num\") == 1) \n",
    "                  .select(\"FlightId\", \"LastUpdate\"))\n",
    "\n",
    "adsb_latest_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flightRadar24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
